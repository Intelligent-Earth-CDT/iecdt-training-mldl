{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch and autodiff basics\n",
    "\n",
    "Adapted from: http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "and CSCS-ICS-DADSi Summer School: Accelerating Data Science with HPC, September 4 - 6, 2017, Swiss National Supercomputing Centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use PyTorch's (http://pytorch.org/) autodifferentiation, and also trains a simple classifier on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors, or multi-dimensional arrays, are the basic unit in PyTorch. Let's create a 5x3 matrix as a 2-dimensional Tensor. This will default to a `FloatTensor`, a tensor of single-precision (32-bits) floating point numbers. The tensor will be initialized with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(3,5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "It is common to have higher-dimensional tensors, to hold all aspects of your data together, such as a 3x200x200 tensor holding the R,G,B channels of an image of 200x200 pixels, or a 100x3x200x200 tensor to hold a batch of 100 such images.\n",
    "\n",
    "We use slicing operations to view specific subtensors of such a tensor. Note that PyTorch uses zero-based indexing and row-major memory ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 images of 3 channels of size 200x200\n",
    "x = torch.Tensor(100,3,200,200)\n",
    "print(x.size())\n",
    "\n",
    "# Second channel of the first image\n",
    "y = x[0,1]\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy conversion\n",
    "\n",
    "You might have noticed that Tensors are very similar to Numpy's ndarrays. In fact you can easily convert between ndarrays and Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.numpy()\n",
    "print(x_np.shape)\n",
    "\n",
    "x = torch.from_numpy(x_np)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on GPU usage\n",
    "\n",
    "PyTorch is frequently used with a CUDA backend on a GPU. One would convert between tensors and CUDA tensors by simply calling\n",
    "\n",
    "`x.cuda()`\n",
    "\n",
    "`x.cpu()`\n",
    "\n",
    "which transfer data to and from a GPU memory.\n",
    "\n",
    "You can check if CUDA is available by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implements reverse-mode automatic differentiation (AD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With AD, obtaining a gradient of a scalar-valued function is straightforward. Let's use the regular Python language features to define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = torch.log(x[0]) + torch.sin(x[1])\n",
    "    return x[2] * torch.exp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the gradient of `f(x)` at `x = [2,3,4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.,3,4], requires_grad=True)\n",
    "y = f(x)\n",
    "print(y)\n",
    "\n",
    "y.backward()# evaluates the gradient of y wrt to the graph leaves (x in this case)\n",
    "print(x.grad)# gradients are accumulated in the .grad attribute of the leaf tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get the function evaluated `f(2,3,4)` as `9.2125` and the gradient `f'(2,3,4)` as `[4.6063, -9.1203, 2.3031]`.\n",
    "\n",
    "Also note the use of `requires_grad = True` for tagging the tensor `x` for being differentiated with respect to. This is for efficiency reasons, allowing PyTorch to ignore the bookkeeping for tensors whose derivatives we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x)\n",
    "y.backward()\n",
    "print(x.grad)# note, the grad attribute is not reset every time .backward is called, \n",
    "# so it can accumulate if you are not careful! (see more here: https://pytorch.org/docs/stable/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two parts, namely defining and using tensors and being able to obtain derivatives, give us everything needed for constructing and training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important point to note is that **neural networks are just functions, and there is nothing special about them.**\n",
    "\n",
    "A neural network is a series of linear algebra operations interleaved with non-linear transformations. Let's create a single feed-forward layer with two neurons connected to four inputs (note that this omits the bias term for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_layer(weights, inputs):\n",
    "    outputs = torch.mm(weights, inputs)\n",
    "    outputs = torch.tanh(outputs)\n",
    "    return outputs\n",
    "\n",
    "W = torch.randn(2,4, requires_grad=True) # A 2x4 weight matrix\n",
    "x = torch.tensor([[1.],[2],[3],[4]]) # A column vector of length four\n",
    "\n",
    "y = run_layer(W, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Training this neural network layer would simply mean taking the derivative of a loss function at its output with respect to its trainable weights (the matrix `W` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(outputs, targets):\n",
    "    return torch.mean((outputs - targets)**2)\n",
    "\n",
    "W = torch.randn(2,4, requires_grad=True) # A 2x4 weight matrix\n",
    "x = torch.tensor([[1.],[2],[3],[4]]) # A column vector of length four\n",
    "\n",
    "loss = loss(run_layer(W, x), torch.tensor([1.,1]))\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "W_grad = W.grad\n",
    "print(W_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "We would then iteratively update the weights in a gradient-based update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "W = W - learning_rate * W_grad\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the gradient of a neural network with respect to its trainable parameters (weights), we can optimize it with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, unless you are implementing new neural network architectures, you would use the existing neural network building blocks provided by the `torch.nn` module, which internally work similar to the basic example we covered above.\n",
    "\n",
    "We will now define a convolutional neural network for image recognition on the MNIST dataset and train and test it, using the regular PyTorch workflow. The MNIST dataset is a collection of 60,000 images of handwritten digits labeled as belonging to one of the ten digit categories.\n",
    "\n",
    "First we load the MNIST dataset. We also apply preprocessing for normalizing data.\n",
    "\n",
    "We divide the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1**: write some code to visualise some of the training examples and their labels.\n",
    "\n",
    "> **Question**: how many examples are there in the training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO: write some code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our neural network composed of two convolutional layers followed by max-pooling layers and two fully-connected (linear) layers.\n",
    "\n",
    "Note that we only need to \n",
    "- create the layer modules (in the `__init__` constructor) and \n",
    "- define the function for the forward run of the network (in the `forward` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # The definition of neural network layers\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x): # The function for forward evaluation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: What are the dimensions of the inputs and outputs of `model(x)`? What are the dimensions of its hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train the network.\n",
    "We use the stochastic gradient descent (SGD) optimization routine provided by the `torch.optim` module. We use a learning rate of `0.0001` and a momentum of `0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)# this automatically batches up examples, adding a batch dimension\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 2**: write two functions to train and test the network. \n",
    "\n",
    "Note: as this is a classification task, we want to pass the output tensor of the network through a softmax function to get class probability predictions, and then use cross entropy for the loss function. I.e., class probability predictions should be computed using `model(x).softmax(dim=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: write some code here\n",
    "    \n",
    "    # pseudocode:\n",
    "    \n",
    "    # for each batch in train_loader\n",
    "        # carry out a forward pass of the network\n",
    "        # compute the loss function (use cross-entropy)\n",
    "        # compute the gradient wrt to the network weights\n",
    "        # carry out an optimisation step (hint: you can use optimizer.step())\n",
    "        # after every 100 batches, print out the current epoch number, batch number and loss value\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: write some code here\n",
    "    \n",
    "    # pseudocode:\n",
    "    \n",
    "    # print out the test loss value averaged over the whole test dataset\n",
    "    # assuming the class with the highest predicted probability as the predicted label,\n",
    "    # print the fraction of correctly predicted labels (accuracy) over the whole test dataset\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the neural network for four epochs. \n",
    "\n",
    "As training progresses, you will see that the classification accuracy in the test set will keep rising. At the end of four epochs, we will have an accuracy of around 85% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some images from the test set. You can evaluate the same cell many times to visualize results for more test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 3**: write some code to visualise the predicted class probabilites for some of the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "> **Extension 1**: change the network design by adding dropout layers after the convolutional layers. How does this affect performance?\n",
    "\n",
    "> **Extension 2**: Use the Adam optimiser instead of SGD, how does this affect performance?\n",
    "\n",
    "> **Extension 3**: How high can the learning rate go before training becomes unstable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
